apiVersion: v1
kind: ConfigMap
metadata:
  name: scale-ci-workload-script
data:
  run.sh: |
    #!/bin/sh
    set -eox pipefail

    workload_log() { echo "$(date -u) $@" >&2; }
    export -f workload_log

    workload_log "Configuring pbench for Prometheus scale"
    mkdir -p /var/lib/pbench-agent/tools-default/
    echo "${USER_NAME:-default}:x:$(id -u):0:${USER_NAME:-default} user:${HOME}:/sbin/nologin" >> /etc/passwd
    if [ "${ENABLE_PBENCH_AGENTS}" = true ]; then
      echo "" > /var/lib/pbench-agent/tools-default/disk
      echo "" > /var/lib/pbench-agent/tools-default/iostat
      echo "workload" > /var/lib/pbench-agent/tools-default/label
      echo "" > /var/lib/pbench-agent/tools-default/mpstat
      echo "" > /var/lib/pbench-agent/tools-default/oc
      echo "" > /var/lib/pbench-agent/tools-default/perf
      echo "" > /var/lib/pbench-agent/tools-default/pidstat
      echo "" > /var/lib/pbench-agent/tools-default/sar
      master_nodes=`oc get nodes -l pbench_agent=true,node-role.kubernetes.io/master= --no-headers | awk '{print $1}'`
      for node in $master_nodes; do
        echo "master" > /var/lib/pbench-agent/tools-default/remote@$node
      done
      infra_nodes=`oc get nodes -l pbench_agent=true,node-role.kubernetes.io/infra= --no-headers | awk '{print $1}'`
      for node in $infra_nodes; do
        echo "infra" > /var/lib/pbench-agent/tools-default/remote@$node
      done
    fi
    source /opt/pbench-agent/profile
    workload_log "Done configuring pbench for Prometheus scale"

    workload_log "Running Prometheus scale workload"
    if [ "${PBENCH_INSTRUMENTATION}" = "true" ]; then
      pbench-user-benchmark --pbench-post='sh /root/workload/post-run.sh' -- sh /root/workload/workload.sh

      if [ "${ENABLE_PBENCH_COPY}" = "true" ]; then
        pbench-move-results --prefix=${PROMETHEUS_SCALE_TEST_PREFIX}
      fi

      # cleanup
      rm -rf /tmp/prometheus_loader.log /tmp/pvc_monitor_0.log /tmp/pvc_monitor_1.log
    else
      sh /root/workload/workload.sh
      RESULT_DIR=/tmp
    fi
    workload_log "Completed Prometheus scale workload run"
  workload.sh: |
    #!/bin/sh
    set -ox pipefail

    db_aging() {
      while true; do
        echo "$(date +'%m-%d-%y-%H:%M:%S') $(oc exec prometheus-k8s-0 -n openshift-monitoring -c prometheus -- df |grep /prometheus$)" >> /tmp/pvc_monitor_0.log
        echo "$(date +'%m-%d-%y-%H:%M:%S') $(oc exec prometheus-k8s-1 -n openshift-monitoring -c prometheus -- df |grep /prometheus$)" >> /tmp/pvc_monitor_1.log
        sleep 120
      done
    }
    export -f db_aging

    # run prometheus loader
    nohup python /root/workload/prometheus-loader.py -g True -i ${PROMETHEUS_REFRESH_INTERVAL} -t ${PROMETHEUS_CONCURRENCY} -p ${PROMETHEUS_GRAPH_PERIOD} &
    loader_pid=$(echo $!)
    
    # db growth monitor
    nohup bash -c db_aging > /dev/null 2>&1 &
    db_aging_pid=$(echo $!)
    
    # sleep x seconds
    sleep ${PROMETHEUS_DURATION};

    # stop the prometheus load
    kill -9 ${loader_pid} ${db_aging_pid}
    
    # test idle
    sleep 300
  post-run.sh: |
    #!/bin/sh
    set -ox pipefail

    RESULT_DIR="/var/lib/pbench-agent/$(ls -t /var/lib/pbench-agent/ | grep "pbench-user" | head -1)"/1/sample1
    echo "Using RESULT_DIR of: \"${RESULT_DIR}\""
    oc logs -n openshift-monitoring prometheus-k8s-0 -c prometheus --since=${PROMETHEUS_DURATION}s > ${RESULT_DIR}/oc_logs_1.log
    oc logs -n openshift-monitoring prometheus-k8s-1 -c prometheus --since=${PROMETHEUS_DURATION}s > ${RESULT_DIR}/oc_logs_2.log
    grep ERROR /tmp/prometheus_loader.log > ${RESULT_DIR}/errors.log
    cat /tmp/prometheus_loader.log |grep duration |grep -v GET |awk '{print $7 " " $13}' |sort > ${RESULT_DIR}/top_longest_queries.log
    mv /tmp/pvc_monitor_0.log ${RESULT_DIR}/
    mv /tmp/pvc_monitor_1.log ${RESULT_DIR}/
  prometheus-loader.py: |
    #!/usr/bin/python
    # -*- coding: utf-8 -*-
    """
    An loader tool to simulate grafana dashboard queries against prometheus.
    
    Usage:
    python prometheusLoader.py
    
    Log:
    /tmp/prometheus_loader.log
    """
    import argparse
    import logging
    import os
    import sys
    import requests
    import random
    import time
    import threading
    from loaddashboards import Dashboards
    from requests.utils import quote
    from concurrent.futures import ThreadPoolExecutor
    
    time_pattern = "YYYY-MM-DD HH:MM:SS.mmm"
    log_file = "/tmp/prometheus_loader.log"
    log_level = "INFO"
    log_format = None
    
    
    def parse_args():
        parser = argparse.ArgumentParser()
        parser.add_argument('-g',
                            '--ignore',
                            type=bool,
                            required=False,
                            default=False,
                            dest='ig',
                            help='ignore templating')
                            
        parser.add_argument('-f',
                            '--file',
                            required=False,
                            dest='file',
                            help='queries file')
    
        parser.add_argument('-t',
                            '--threads',
                            required=False,
                            type=int,
                            default=20,
                            dest='threads',
                            help='simultaneously requests')
    
        parser.add_argument('-i',
                            '--interval',
                            type=int,
                            required=False,
                            default=10,
                            dest='interval',
                            help='sleep interval for each block iteration in sec')
    
        parser.add_argument('-p',
                            '--period',
                            type=int,
                            required=False,
                            default=60,
                            dest='period',
                            help='a time period for query in min')
    
        parser.add_argument('-r',
                            '--resolution',
                            type=int,
                            required=False,
                            dest='resolution',
                            help='graph resolution in seconds')
    
        parser.add_argument('-n',
                            '--namespace',
                            type=str,
                            required=False,
                            default='openshift-monitoring',
                            dest='ns',
                            help='promethues namespace')
    
        parser.add_argument('-s',
                            '--sa',
                            type=str,
                            required=False,
                            default='prometheus-k8s',
                            dest='sa',
                            help='promethues service_account')
    
        parser.add_argument('-y',
                            '--yaml',
                            type=str,
                            required=False,
                            default='https://raw.githubusercontent.com/openshift/cluster-monitoring-operator/master/assets/grafana/dashboard-definitions.yaml',
                            dest='yaml',
                            help='gitrepo')
    
        return parser.parse_args()
    
    class PrometheusLoader(object):
        def __init__(self, file, threads, period, resolution, ns, sa, yaml,
                    interval, ig):
            # args
            self.threads = threads
            self.period = period
            self.queries = []
            self.token = None
            self.promethues_server = None
            self.executor = ThreadPoolExecutor(max_workers=threads)
            self.headers = None
            self.log = None
            self.log_level = log_level
            self.log_format = "%(asctime)s - %(levelname)s - %(message)s"
            self.log_file = log_file
            self.pattern = time_pattern
            self.steping = resolution
            self.query = ""
            self.ns = ns
            self.sa = sa
            self.dashboardname = ""
            self.con = 0
            self.interval = interval
            self.ignore_templats = ig
            self.logger()
            self.get_prometheus_info()
            self.lock = threading.Lock()
    
            # stepping computation
            if resolution is None:
                self.steping = self.compute_stepping()
            else:
                self.steping = resolution
    
            # dashboard loader
            if yaml:
                self.load_queries_from_source(yaml)
            elif file:
                self.load_queries_from_file(file)
            else:
                self.log.error('yaml or file should given')
                sys.exit(os.EX_CONFIG)
    
        def logger(self):
            try:
                logging.basicConfig(filename=self.log_file, filemode='w',
                                    level='DEBUG', format=self.log_format)
                self.log = logging.getLogger(self.log_file)
                self.log.info('starting')
            except Exception as e:
                self.log.error('failed to start log {0} - {1}'.format(
                    self.log_file, e))
    
        def load_queries_from_file(self, file):
            ''' load and qute queries from text file '''
            with open(file, 'r') as f:
                for line in f:
                    self.queries.append(quote(line))
    
        def load_queries_from_source(self, yaml):
            ''' load and qute queries from source (yaml mixin) '''
            self.dashboards = Dashboards(yaml, self.ignore_templats).get_dashboards()
    
        def get_prometheus_info(self):
            ''' get token and route for prometeus '''
            self.token = 'Bearer ' + os.popen('oc sa get-token {0}'
                                              ' -n {1}'.format(self.sa, self.ns)
                                              ).read()
    
            self.headers = {'Authorization': self.token,
                            'Accept': 'application/json, text/plain, */*',
                            'Accept-Encoding': 'gzip, deflate, br',
                            'Connection': 'keep-alive',
                            'X-Grafana-Org-Id': '1'
                           }
    
            self.promethues_server = os.popen("oc get route {0}"
                                              " -n {1}".format(self.sa, self.ns) +
                                              " |grep prometheus |awk '{print $2}'"
                                              ).read().rstrip()
    
    
        def generate_req(self, q):
            ''' generate query as http format '''
    
            time_from = os.popen('date "+%s" -d "{0} min ago"'.format(
                                    self.period)).read().rstrip()
            time_now = os.popen('date "+%s"').read().rstrip()
            # lock the q(query) in order to sync the log.
            self.lock.acquire()
            self.query = q
            return "https://{0}/api/v1/query_range?query={1}&start={2}&end={3}" \
                "&step={4}".format(self.promethues_server,
                                 q, time_from, time_now, self.steping)
    
        def request(self, req, health=False):
            ''' fire http request '''
            reqinfo = ' [{0}] - concurrency:{1} - query:{2}'.format(
                                                self.dashboardname,
                                                self.con,
                                                self.query)
            # release the lock.
            self.lock.release()
            try:
                res = requests.get(req, verify=False, headers=self.headers)
            except (IOError, RequestException) as e:
                self.log.error('bad request {0} response {1}'.format(reqinfo, e))
            if len(res.text) == 0:
                self.log.error('bad request {0} response {1}'.format(reqinfo, res))
            self.log.info('duration: {0} - {1}'.format(res.elapsed.total_seconds(),
                                                    reqinfo))
            if health:
                pass
                # TODO: log the health from the query results set
    
        def run_loader(self, queries):
            ''' fire http requests simultaneously in threads batch '''
            for query in queries:
                self.executor.submit(self.request, self.generate_req(query))
    
        def health_collector(self):
            # check promethues scrapping success rate
            self.executor.submit(self.request, 'topk(10, rate(up[10m]))')
            self.executor.submit(self.request, 'sum(scrape_samples_scraped)')
            #self.executor.submit(self.request, 'example of kube-state query')
    
        def compute_stepping(self):
            ''' run once.
            compute the step size for high resolution
            '''
            last3, last6, last12, last24, lastwk = 180, 360, 720, 1440, 10080
            if self.period >= 60 and self.period < last3:
                return 30
            elif self.period > last3 and self.period < last6:
                return 40
            elif self.period > last6 and self.period < last12:
                return 60
            elif self.period > last12 and self.period < last24:
                return 120
            elif self.period > lastwk: # last week
                return 1200
            else:
                return 15
    
        def dashboard_loader(self):
            if self.queries:
                self.run_loader(self.queries)
                time.sleep(self.interval)
            elif self.dashboards:
                # iterate all dashboards with pause of interval
                for dashboard in self.dashboards:
                    self.dashboardname = dashboard['name']
                    self.con = len(dashboard['queries'])
                    if self.con == 0:
                        continue
                    self.executor = ThreadPoolExecutor(max_workers=self.con)
                    self.run_loader(dashboard['queries'])
                    time.sleep(self.interval)
            else:
                self.log.error('no dashboard or queries were found')
                sys.exit(os.EX_CONFIG)
    
        # start the loader.
        def start(self):
            while True:
                self.dashboard_loader()
                self.health_collector()
    
    if __name__ == "__main__":
        args = parse_args()
        p = PrometheusLoader(args.file, args.threads, args.period, args.resolution,
                             args.ns, args.sa, args.yaml, args.interval, args.ig)
        # start the loader.
        p.start()
  __init__.py:
  loaddashboards.py: |
    #!/usr/bin/python
    # -*- coding: utf-8 -*-
    """
    The following module will load and parse the dashboard-definitions from
    cluster-monitoring-operator project and retrieve dashboards object that include
    a set of dashboard and queries dictionaries.
    """
    import yaml
    import json
    import os
    import argparse
    from requests.utils import quote
    
    
    def parse_args():
        parser = argparse.ArgumentParser()
        parser.add_argument('-g',
                            '--gitrepo',
                            type=str,
                            required=False,
                            default='https://raw.githubusercontent.com/openshift/cluster-monitoring-operator/master/assets/grafana/dashboard-definitions.yaml',
                            dest='gitrepo',
                            help='gitrepo')
        parser.add_argument('-p',
                            '--print',
                            type=str,
                            required=False,
                            dest='printit',
                            help='print results')
        parser.add_argument('-i',
                            '--ignore',
                            type=bool,
                            required=False,
                            default=False,
                            dest='ignore',
                            help='ignore templating')
        return parser.parse_args()
    
    class Dashboards(object):
        def __init__(self, file, ignore=False):
            self.dashboards = []
            self.ignore = ignore
    
            if 'http' in file:
                import urllib
                sock = urllib.urlopen(file)
                htmlSource = sock.read()
                sock.close()
                self.yaml = yaml.load(htmlSource)
            else:
                self.yaml = self.load_yaml(file)
    
            self.scan_dashboards()
    
        def load_yaml(self, f):
            with open(f, 'r') as y:
                return yaml.load(y)
    
        def scan_dashboards(self):
            for item in self.yaml.get('items'):
                dashboard = {}
                dashboard['name'] = item['data'].keys()[0]
                dashboard['queries'] = self.scan_queries(json.loads(item['data'].values()[0]))
                self.dashboards.append(dashboard)
    
        def scan_queries(self, jsondata):
            exprs = []
            for raw in jsondata['rows']:
                for panel in raw['panels']:
                    for target in panel['targets']:
                        t = target['expr']
                        # TODO: // make sure to have templating values
                        if self.ignore:
                            if 'node=\"$node\"' in t:
                                t = t.replace('node=\"$node\"','node=~"^.*"')
                            if 'job="$cluster"' in t:
                                t = t.replace('job="$cluster"','job=~".*"')
                            if 'namespace=\"$namespace\"' in t:
                                t = t.replace('namespace=\"$namespace\"','namespace=~"^.*"')
                            if 'pod_name=\"$pod\"' in t:
                                t = t.replace('pod_name=\"$pod\"','pod_name=~"^.*"')
                            if 'instance=\"$instance\"' in t:
                                t = t.replace('instance=\"$instance\"','instance=~"^.*"')
                            if 'statefulset=\"$statefulset\"' in t:
                                t = t.replace('statefulset=\"$statefulset\"','statefulset=~"^.*"')
                        exprs.append(quote(t))
            return exprs
    
        def get_dashboards(self):
            return self.dashboards
